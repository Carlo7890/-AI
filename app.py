import streamlit as st
import pandas as pd
import re
import requests
import base64
import json

# CSV ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    words_df = pd.read_csv("ì‚¬ê³ ë„êµ¬ì–´_ë‹¨ì–´ë“±ê¸‰ë§¤í•‘.csv", encoding='euc-kr')
    score_df = pd.read_csv("ì˜¨ë…ì§€ìˆ˜ë²”ìœ„.csv", encoding='utf-8')
    score_df[['min', 'max']] = score_df['ì˜¨ë…ì§€ìˆ˜ ë²”ìœ„'].str.split('~', expand=True).astype(int)
    return words_df, score_df

# ì˜¨ë…ì§€ìˆ˜ ê³„ì‚° (LLaMA3 ì¶”ì¶œ ê¸°ë°˜)
def calculate_ondok_score_from_words(matched_df, score_df):
    total = len(matched_df)
    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    weighted = sum({1: 4, 2: 3, 3: 2, 4: 1}.get(row["ë“±ê¸‰"], 1) for _, row in matched_df.iterrows())
    score = min(280, (weighted / (4 * total)) * 280)

    for _, row in score_df.iterrows():
        if row['min'] <= score <= row['max']:
            return round(score), row['ëŒ€ìƒ í•™ë…„']
    return round(score), "í•´ì„ ë¶ˆê°€"

# LLaMA3 ì‚¬ê³ ë„êµ¬ì–´ ì¶”ì¶œ (CSV ê¸°ë°˜ + ì¤‘ì˜ì–´ ì²˜ë¦¬)
def llama3_extract_csv_concepts(text, word_list):
    headers = {
        "Authorization": f"Bearer {st.secrets['groq_api_key']}",
        "Content-Type": "application/json"
    }
    ambiguous_guide = """
    ë‹¤ìŒ ë‹¨ì–´ë“¤ì€ ë¬¸ë§¥ì— ë”°ë¼ ë“±ê¸‰ì´ ë‹¤ë¥´ë¯€ë¡œ, ë¬¸ì¥ì—ì„œ ì–´ë–¤ ì˜ë¯¸ë¡œ ì“°ì˜€ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ì—¬ í•´ë‹¹ ì˜ë¯¸ì™€ ë“±ê¸‰ì„ ì„ íƒí•˜ì„¸ìš”:
    - ê¸°ìˆ : 2ë“±ê¸‰ (ê¸°ëŠ¥/ë°©ë²•), 3ë“±ê¸‰ (ê¸°ë¡/ì„œìˆ )
    - ìœ í˜•: 2ë“±ê¸‰ (ê°ˆë˜), 3ë“±ê¸‰ (í˜•ìƒ)
    - ì˜ì§€: 2ë“±ê¸‰ (ê²°ì‹¬), 3ë“±ê¸‰ (ê¸°ëŒ€ë‹¤)
    - ì§€ì : 2ë“±ê¸‰ (ì§€ì‹œ), 3ë“±ê¸‰ (ì§€ì„±)
    """
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì‚¬ê³ ë„êµ¬ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ ëª©ë¡ì— í¬í•¨ëœ ë‹¨ì–´ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    {', '.join(word_list)}

    {ambiguous_guide}

    ì¶œë ¥ì€ í‘œ í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•˜ì„¸ìš”:
    ë²ˆí˜¸ / ë‹¨ì–´ / ë“±ê¸‰ / ì„ íƒí•œ ì˜ë¯¸ / ë¹„ìŠ·í•œ ë§ / ë°˜ëŒ€ë§
    ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì„¸ìš”.

    ë¬¸ì¥:
    {text}
    """
    payload = {
        "model": "llama3-70b-8192",
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    }
    try:
        response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload)
        return response.json()['choices'][0]['message']['content']
    except:
        return "LLaMA3 API í˜¸ì¶œ ì˜¤ë¥˜"

# Google Vision OCR

def image_to_text_google_vision(image_file):
    api_key = st.secrets["google_api_key"]
    content = base64.b64encode(image_file.read()).decode("utf-8")
    body = json.dumps({
        "requests": [{
            "image": {"content": content},
            "features": [{"type": "TEXT_DETECTION"}]
        }]
    })
    response = requests.post(
        f"https://vision.googleapis.com/v1/images:annotate?key={api_key}",
        headers={"Content-Type": "application/json"},
        data=body
    )
    try:
        return response.json()['responses'][0]['fullTextAnnotation']['text']
    except:
        return ""

# Streamlit ì•± ì‹œì‘
st.title("ğŸ“š ì˜¨ë…AI: LLaMA3 ê¸°ë°˜ ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ë° ë…ì„œì§€ìˆ˜")

image_file = st.file_uploader("ğŸ“· ë˜ëŠ” ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)", type=['png', 'jpg', 'jpeg', 'heic'])
if image_file:
    extracted_text = image_to_text_google_vision(image_file)
    st.text_area("ğŸ“ OCR ì¶”ì¶œ ê²°ê³¼:", value=extracted_text, height=150, key="ocr_output")
    text_input = extracted_text
else:
    text_input = st.text_area("âœï¸ ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

run_button = st.button("ğŸ” ë¶„ì„í•˜ê¸°")

if run_button and text_input:
    words_df, score_df = load_data()
    word_list = words_df['ë‹¨ì–´'].tolist()

    st.markdown("---")
    st.subheader("ğŸ§  LLaMA3 ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ê²°ê³¼")
    llama_output = llama3_extract_csv_concepts(text_input, word_list)
    st.write(llama_output)

    # CSV ê¸°ë°˜ ë‹¨ì–´ë§Œ ì¶”ì¶œ
    found_words = [word for word in word_list if word in text_input]
    matched_df = words_df[words_df['ë‹¨ì–´'].isin(found_words)].copy()
    matched_df.insert(0, 'ë²ˆí˜¸', range(1, len(matched_df) + 1))

    score, level = calculate_ondok_score_from_words(matched_df, score_df)
    st.success(f"ğŸ§  ì˜¨ë…ì§€ìˆ˜: {score}ì ")
    st.info(f"ğŸ“ ì¶”ì • í•™ë…„ ìˆ˜ì¤€: {level}")
    if not matched_df.empty:
        st.dataframe(matched_df.set_index('ë²ˆí˜¸')[['ë‹¨ì–´', 'ë“±ê¸‰']])

