import streamlit as st
import pandas as pd
import re
import requests
import base64
import json

@st.cache_data
def load_data():
    words_df = pd.read_csv("ì‚¬ê³ ë„êµ¬ì–´_ë‹¨ì–´ë“±ê¸‰ë§¤í•‘.csv", encoding='euc-kr')
    score_df = pd.read_csv("ì˜¨ë…ì§€ìˆ˜ë²”ìœ„.csv", encoding='utf-8')
    score_df[['min', 'max']] = score_df['ì˜¨ë…ì§€ìˆ˜ ë²”ìœ„'].str.split('~', expand=True).astype(int)
    return words_df, score_df

def calculate_ondok_score_from_words(matched_df, score_df):
    total = len(matched_df)
    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    weighted = sum({1: 4, 2: 3, 3: 2, 4: 1}.get(row["ë“±ê¸‰"], 1) for _, row in matched_df.iterrows())
    score = min(280, (weighted / (4 * total)) * 280)
    for _, row in score_df.iterrows():
        if row['min'] <= score <= row['max']:
            return round(score), row['ëŒ€ìƒ í•™ë…„']
    return round(score), "í•´ì„ ë¶ˆê°€"

def llama3_extract_csv_concepts(text, word_list):
    headers = {
        "Authorization": f"Bearer {st.secrets['groq_api_key']}",
        "Content-Type": "application/json"
    }
    ambiguous_guide = """
    ë‹¤ìŒ ë‹¨ì–´ë“¤ì€ ë¬¸ë§¥ì— ë”°ë¼ ë“±ê¸‰ì´ ë‹¤ë¥´ë¯€ë¡œ, ë¬¸ì¥ì—ì„œ ì–´ë–¤ ì˜ë¯¸ë¡œ ì“°ì˜€ëŠ”ì§€ë¥¼ íŒë‹¨í•˜ì—¬ í•´ë‹¹ ì˜ë¯¸ì™€ ë“±ê¸‰ì„ ì„ íƒí•˜ì„¸ìš”:
    - ê¸°ìˆ : 2ë“±ê¸‰ ê¸°ìˆ 1: ì‚¬ë¬¼ì„ ì˜ ë‹¤ë£° ìˆ˜ ìˆëŠ” ë°©ë²•ì´ë‚˜ ëŠ¥ë ¥(ê¸°ëŠ¥/ë°©ë²•).
            3ë“±ê¸‰ ê¸°ìˆ 3: ì—´ê±°í•˜ê±°ë‚˜ ê¸°ë¡í•˜ì—¬ ì„œìˆ í•¨(ê¸°ë¡/ì„œìˆ ).
    - ìœ í˜•: 2ë“±ê¸‰ ìœ í˜•7: ì„±ì§ˆì´ë‚˜ íŠ¹ì§• ë”°ìœ„ê°€ ê³µí†µì ì¸ ê²ƒë¼ë¦¬ ë¬¶ì€ í•˜ë‚˜ì˜ í‹€(ê°ˆë˜).
            3ë“±ê¸‰ ìœ í˜•2: í˜•ìƒì´ë‚˜ í˜•ì²´ê°€ ìˆìŒ.
    - ì˜ì§€: 2ë“±ê¸‰ ì˜ì§€6: ì–´ë– í•œ ì¼ì„ ì´ë£¨ê³ ì í•˜ëŠ” ë§ˆìŒ(ê²°ì‹¬).
            3ë“±ê¸‰ ì˜ì§€4: ë‹¤ë¥¸ ê²ƒì— ëª¸ì„ ê¸°ëŒ€ê±°ë‚˜ ë§ˆìŒì„ ê¸°ëŒ€ì–´ ë„ì›€ì„ ë°›ìŒ.
    - ì§€ì : 2ë“±ê¸‰ ì§€ì 5: ê¼­ ì§‘ì–´ì„œ ê°€ë¦¬í‚´(ì§€ëª©/ì§€ì‹œ).
            3ë“±ê¸‰ ì§€ì 1: ì§€ì‹ì´ë‚˜ ì§€ì„±ì— ê´€í•œ ê²ƒ(ì§€ì„±ì ).
    """
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì‚¬ê³ ë„êµ¬ì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ ëª©ë¡ì— í¬í•¨ëœ ë‹¨ì–´ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    {', '.join(word_list)}

    {ambiguous_guide}

    ì¶œë ¥ì€ í‘œê°€ ì•„ë‹Œ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì˜ˆ: 'ê¸°ìˆ /2', 'ìœ í˜•/3'
    ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤. ì˜ì–´ê°€ ì„ì—¬ ìˆìœ¼ë©´ ì˜¤ë‹µ ì²˜ë¦¬ë©ë‹ˆë‹¤.

    ë¬¸ì¥:
    {text}
    """
    payload = {
        "model": "llama3-70b-8192",
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    }
    try:
        response = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload)
        return response.json()['choices'][0]['message']['content']
    except:
        return "LLaMA3 API í˜¸ì¶œ ì˜¤ë¥˜"

def image_to_text_google_vision(image_file):
    api_key = st.secrets["google_api_key"]
    content = base64.b64encode(image_file.read()).decode("utf-8")
    body = json.dumps({
        "requests": [{
            "image": {"content": content},
            "features": [{"type": "TEXT_DETECTION"}]
        }]
    })
    response = requests.post(
        f"https://vision.googleapis.com/v1/images:annotate?key={api_key}",
        headers={"Content-Type": "application/json"},
        data=body
    )
    try:
        return response.json()['responses'][0]['fullTextAnnotation']['text']
    except:
        return ""

st.title("ğŸ“š ì˜¨ë…AI: LLaMA3 ê¸°ë°˜ ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ë° ë…ì„œì§€ìˆ˜")

image_file = st.file_uploader("ğŸ“· ë˜ëŠ” ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)", type=['png', 'jpg', 'jpeg', 'heic'])
if image_file:
    extracted_text = image_to_text_google_vision(image_file)
    st.text_area("ğŸ“ OCR ì¶”ì¶œ ê²°ê³¼:", value=extracted_text, height=150, key="ocr_output")
    text_input = extracted_text
else:
    text_input = st.text_area("âœï¸ ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

run_button = st.button("ğŸ” ë¶„ì„í•˜ê¸°")

if run_button and text_input:
    words_df, score_df = load_data()
    word_list = words_df['ë‹¨ì–´'].tolist()

    llama_output = llama3_extract_csv_concepts(text_input, word_list)

    matched_words = []
    for line in re.split(r'[,
]', llama_output):
        parts = line.strip().split('/')
        if len(parts) == 2:
            word = parts[0].strip()
            try:
                grade = int(parts[1])
                match = words_df[(words_df['ë‹¨ì–´'] == word) & (words_df['ë“±ê¸‰'] == grade)]
                matched_words.append(match)
            except:
                continue

    if any(not df.empty for df in matched_words):
        matched_df = pd.concat([df for df in matched_words if not df.empty]).drop_duplicates().reset_index(drop=True)
        score, level = calculate_ondok_score_from_words(matched_df, score_df)
        st.markdown("---")
        st.success(f"ğŸ§  ì˜¨ë…ì§€ìˆ˜: {score}ì ")
        st.info(f"ğŸ“ ì¶”ì • í•™ë…„ ìˆ˜ì¤€: {level}")
        st.markdown("---")
        st.subheader("ğŸ” ì‚¬ìš©ëœ ì‚¬ê³ ë„êµ¬ì–´")
        used_list = [f"{row['ë‹¨ì–´']}/{row['ë“±ê¸‰']}ë“±ê¸‰" for _, row in matched_df.iterrows()]
        st.write(', '.join(used_list))
    else:
        st.warning("ì‚¬ê³ ë„êµ¬ì–´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ì–´ìš”.")
