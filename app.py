import streamlit as st
import pandas as pd
import re
import requests
import base64
import json

# CSV ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    words_df = pd.read_csv("ì‚¬ê³ ë„êµ¬ì–´_ë‹¨ì–´ë“±ê¸‰ë§¤í•‘.csv", encoding='euc-kr')
    score_df = pd.read_csv("ì˜¨ë…ì§€ìˆ˜ë²”ìœ„.csv", encoding='utf-8')
    score_df[['min', 'max']] = score_df['ì˜¨ë…ì§€ìˆ˜ ë²”ìœ„'].str.split('~', expand=True).astype(int)
    return words_df, score_df

# ì •í™•í•œ ì¼ì¹˜ ë‹¨ì–´ë§Œ ì¶”ì¶œ
def extract_exact_words(text, word_list):
    tokens = re.findall(r"[\wê°€-í£]+", text)
    return [word for word in tokens if word in word_list]

# ì˜¨ë…ì§€ìˆ˜ ê³„ì‚° ê°œì„  ë²„ì „ (CTTR, ë°€ë„ ë°˜ì˜)
def calculate_ondok_score_advanced(text, matched_df, grade_ranges):
    seen = set(matched_df['ë‹¨ì–´'].tolist())
    total = len(matched_df)
    if total == 0:
        return 0, "ì‚¬ê³ ë„êµ¬ì–´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", [], 0, 0

    word_tokens = re.findall(r"[\wê°€-í£]+", text)
    cttr = min(len(seen) / (2 * total) ** 0.5, 1.0)
    weighted = sum({1: 4, 2: 3, 3: 2, 4: 1}[g] for g in matched_df['ë“±ê¸‰'])
    norm_weight = weighted / (4 * total)
    density = total / len(word_tokens)
    index = ((0.7 * cttr + 0.3 * norm_weight) * 500 + 100) * (0.5 + 0.5 * density)
    if len(word_tokens) < 5:
        index *= 0.6

    matched = [g for s, e, g in grade_ranges if s <= index < e]
    level = "~".join(matched) if matched else "í•´ì„ ë¶ˆê°€"
    return round(index), level, seen, total, len(word_tokens)

# Groq ê¸°ë°˜ LLaMA3 ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ê¸°ëŠ¥

def llama3_extract_concepts(text):
    headers = {
        "Authorization": f"Bearer {st.secrets['groq_api_key']}",
        "Content-Type": "application/json"
    }
    prompt = f"""
    ë‹¤ìŒ ê¸€ì—ì„œ ì‚¬ê³ ë„êµ¬ì–´ë¥¼ ì¶”ì¶œí•´ì¤˜. ê° ë‹¨ì–´ë§ˆë‹¤ ë‹¤ìŒ ì •ë³´ë¥¼ í‘œ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:
    ë²ˆí˜¸ / ë‹¨ì–´ / ë“±ê¸‰(1~4 ì¤‘ ì˜ˆìƒ) / ì‚¬ì „ì  ì˜ë¯¸ / ë¹„ìŠ·í•œ ë§ / ë°˜ëŒ€ë§

    ë¬¸ì¥:
    {text}

    ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œê¸€ë¡œ ì¶œë ¥í•´ì¤˜.
    """
    payload = {
        "model": "llama3-70b-8192",
        "messages": [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì´ˆë“±í•™ìƒì„ ìœ„í•œ ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]
    }
    try:
        res = requests.post("https://api.groq.com/openai/v1/chat/completions", headers=headers, json=payload)
        return res.json()['choices'][0]['message']['content']
    except:
        return "LLaMA3 ìš”ì•½ ì‹¤íŒ¨ ë˜ëŠ” í‚¤ ì˜¤ë¥˜"

# Google Vision OCR

def image_to_text_google_vision(image_file):
    api_key = st.secrets["google_api_key"]
    content = base64.b64encode(image_file.read()).decode("utf-8")
    body = json.dumps({
        "requests": [{
            "image": {"content": content},
            "features": [{"type": "TEXT_DETECTION"}]
        }]
    })
    response = requests.post(
        f"https://vision.googleapis.com/v1/images:annotate?key={api_key}",
        headers={"Content-Type": "application/json"},
        data=body
    )
    try:
        return response.json()['responses'][0]['fullTextAnnotation']['text']
    except:
        return ""

# Streamlit ì•± ì‹œì‘
st.title("ğŸ“š ì˜¨ë…AI: ì‚¬ê³ ë„êµ¬ì–´ ê¸°ë°˜ ë…ì„œì§€ìˆ˜ ë¶„ì„")

image_file = st.file_uploader("ğŸ“· ë˜ëŠ” ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)", type=['png', 'jpg', 'jpeg', 'heic'])
if image_file:
    extracted_text = image_to_text_google_vision(image_file)
    st.text_area("ğŸ“ OCR ì¶”ì¶œ ê²°ê³¼:", value=extracted_text, height=150, key="ocr_output")
    text_input = extracted_text
else:
    text_input = st.text_area("âœï¸ ë¶„ì„í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:")

run_button = st.button("ğŸ” ë¶„ì„í•˜ê¸°")

if run_button and text_input:
    words_df, score_df = load_data()
    word_list = words_df['ë‹¨ì–´'].tolist()
    used_words = extract_exact_words(text_input, word_list)
    matched_df = words_df[words_df['ë‹¨ì–´'].isin(used_words)].copy()

    if matched_df.empty:
        st.warning("ì‚¬ê³ ë„êµ¬ì–´ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ì–´ìš”.")
    else:
        grade_ranges = [(row['min'], row['max'], row['ëŒ€ìƒ í•™ë…„']) for _, row in score_df.iterrows()]
        score, level, seen_words, total_used, total_words = calculate_ondok_score_advanced(text_input, matched_df, grade_ranges)

        st.success(f"ğŸ§  ì˜¨ë…ì§€ìˆ˜: {score:.1f}ì ")
        st.info(f"ğŸ“ ì¶”ì • í•™ë…„ ìˆ˜ì¤€: {level}")

        # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ë²ˆí˜¸ ì—´ ì¶”ê°€
        display_df = matched_df[['ë‹¨ì–´', 'ë“±ê¸‰']].copy()
        display_df.insert(0, 'ë²ˆí˜¸', range(1, len(display_df) + 1))
        st.dataframe(display_df.set_index('ë²ˆí˜¸'))

        st.markdown("---")
        st.subheader("ğŸ§  LLaMA3 ì‚¬ê³ ë„êµ¬ì–´ ë¶„ì„ ê²°ê³¼")
        st.write(llama3_extract_concepts(text_input))
